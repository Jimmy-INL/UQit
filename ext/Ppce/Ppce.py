###############################################################
# Probabilistic generalized Polynomial Chaos Expansion (PgPCE)
#     PPCE = gPCE + GPR
###############################################################
#--------------------------------------------------------------
# Saleh Rezaeiravesh, salehr@kth.se
#--------------------------------------------------------------
"""
   >>> Probabilistic generalized Polynomial Chaos Expansion (PgPCE) 
   [IDEA]:
   We have a true unobserved simulator f(q). We would like to estimate statisical moments of f(q) where q are RV allowed to vary over an admissible space with some distribution. For this probem our appraoch is to use gPCE. However, the training outputs generated by f(q) can be uncertain themselves. To handle this we need to combine gPCE and GPR-surrogates. By solving this problem, we make estimations for statistical moments of f(q) which are now random!
   [Required Steps]:
   1. Construct a GPR-surrogate for f(q) (shown by g(q)) based on limited number of noisy training data: y=g(q)+e
      - Through this, the uncertainty in each observation is taken into account in the GPR surrogate.
      - GPR supports both homoscedastic and heteroscedastic noises
      - GPR is built using GPyTorch
   2. Use gPCE to estimate statisical moments of the surrogate (and hence actual f(q)) due to the variablity of the q over an admissible space.     
      - To gPCE is constructed based on Gauss-Quadrature technique. Since it is efficient, considering the low expense of drawing samples from GPR surrogate.
      - As a result of PgPCE, estimates for mean and variance of g(q) are random variables.
        To construct their dstribution, we do brute force MC.
        This means, we repeat m times estimating the gPCE moments based on Gauss quadrature samples taken from the GPR.
   NOTE: now only works for uniform q!     
"""
#
import os
import sys
import numpy as np
import math as mt
import matplotlib
import matplotlib.pyplot as plt
myUQtoolboxPATH=os.getenv("myUQtoolboxPATH")
sys.path.append(myUQtoolboxPATH+'/gPCE/')
sys.path.append(myUQtoolboxPATH+'/ext/gpr/')
sys.path.append(myUQtoolboxPATH+'/pdfHisto/')
sys.path.append(myUQtoolboxPATH+'/analyticFuncs/')
sys.path.append(myUQtoolboxPATH+'/writeUQ/')
import gpce
import gpr_torch
import pdfHisto
import analyticTestFuncs
import writeUQ
#
#/////////////////////////////////////////////////////////////
def Ppce_LegUnif_1d_cnstrct(qTrain,yTrain,noiseSdev,PpceDict):
    """
       Probabistric PCE (gPCE+GPR) for 1d-input parameter, y=f(q)+e
       Inputs:
           qTrain: training input parameters q, 1d numpy array of size n
           yTrain: training mean observation y, 1d numpy array of size n
           noiseSdev: noise sdev of training observations: e~N(0,noiseSdev), 1d numpy array of size n
           PpceDict: dictionary containing controllers for PPCE, including:
                nGQ: number of GQ test points 
                qBound: admissible range of q
                nMC: number of independent samples drawn from GPR to construct PCE 
                nIter_gpr: number of iterations for optimization of GPR hyper-parameters
                lr_gpr: learning rate for optimization of GPT hyper-parameters
    """
    #(0) assignments
    nGQ=PpceDict['nGQtest']       #number of GQ test points
    qBound=PpceDict['qBound'] #admissible range of inputs parameter
    nMC=PpceDict['nMC']       #number of samples taken from GPR
    #make a dict for gpr
    gprOpts={'nIter':PpceDict['nIter_gpr'],    #number of iterations to optimize hyperparameters
             'lr':PpceDict['lr_gpr']           #learning rate in opimization of hyperparameters
            }

    #(1) generate test points that are Gauss quadratures chosen based on the distribution of q (gPCE rule) 
    xiGQ,wGQ=gpce.GaussLeg_ptswts(nGQ)  #xiGQ\in[-1,1]
    qTest=gpce.mapFromUnit(xiGQ,qBound) #qTest\in qBound

    #(2) Construct GPR surrogate based on training data
    post_f,post_obs=gpr_torch.gprTorch_1d(qTrain,[yTrain],noiseSdev,qTest,gprOpts)

    #(3) Use samples of GPR tested at GQ nodes to construct a PCE
    #    nMC independent samples are drawn from the GPR surrogate
    fMean_list=[]      #list of estimates for E[f(q)] 
    fVar_list =[]      #list of estimates for V[f(q)]
    for j in range(nMC):
        # draw a sample for f(q) from GPR surrogate
        f_=post_obs.sample().numpy()
        # construct PCE for the drawn sample
        fCoef_,fMean_,fVar_=gpce.pce_LegUnif_1d_cnstrct(f_)
        fMean_list.append(fMean_)
        fVar_list.append(fVar_)

    #(4) convert lists to numpy arrays    
    # estimates for their mean and sdev: fMean_list.mean(), fMean_list.std(), ...
    fMean_list=np.asarray(fMean_list)
    fVar_list=np.asarray(fVar_list)
    #optional outputs: only used for plot in the test below
    #in general we do not need them
    optOut={'post_f':post_f,'post_obs':post_obs,'qTest':qTest}
    return fMean_list,fVar_list,optOut



###############################
# External Functions for Test
###############################
import torch   #for plot
def Ppce_LegUnif_1d_cnstrct_test():
    """
        Test for Ppce_LegUnif_1d_cnstrct()
    """
    def fEx(x):
        """
           Exact simulator
        """
        #yEx=np.sin(2*mt.pi*x)
        yEx=analyticTestFuncs.fEx1D(x)
        return yEx
    #
    def noiseGen(n,noiseType):
        """
           Generate a 1D numpy array of standard deviations of independent Gaussian noises
        """
        if noiseType=='homo': #homoscedastic noise 
           sd=0.1   #standard deviation (NOTE: cannot be zero, but can be very small)
           sdV=[sd]*n
           sdV=np.asarray(sdV)
        elif noiseType=='hetero': #heteroscedastic noise
           sdMin=0.02
           sdMax=0.2
           sdV=sdMin+(sdMax-sdMin)*np.linspace(0.0,1.0,n)
        return sdV  #vector of standard deviations
    #
    def trainData(xBound,n,noiseType):
        """
          Create training data D={X,Y}
        """
        x=np.linspace(xBound[0],xBound[1],n)
        sdV=noiseGen(n,noiseType)
        y=fEx(x) + sdV * np.random.randn(n)
        return x,y,sdV
    #
    def gpr1D_plotter(post_f,post_obs,xTrain,yTrain,xTest,fExTest):
        """
           Plot GPR constructed by GPyToch for 1D input
        """
        with torch.no_grad():
             lower_f, upper_f = post_f.confidence_region()
             lower_obs, upper_obs = post_obs.confidence_region()
             plt.figure(figsize=(10,6))
             plt.plot(xTest,fExTest,'--b',label='Exact Output')
             plt.plot(xTrain, yTrain, 'ok',markersize=4,label='Training observations')
             plt.plot(xTest, post_f.mean[:].numpy(), '-r',lw=2,label='Mean Model')
             plt.plot(xTest, post_obs.mean[:].numpy(), ':m',lw=2,label='Mean Posterior Prediction')
             plt.plot(xTest, post_obs.sample().numpy(), '-k',lw=1,label='Sample Posterior Prediction')
             plt.fill_between(xTest, lower_f.numpy(), upper_f.numpy(), alpha=0.3,label='Confidence f(q)')
             plt.fill_between(xTest, lower_obs.numpy(), upper_obs.numpy(), alpha=0.15, color='r',label='Confidence Yobs')
             plt.legend(loc='best',fontsize=15)
             #NOTE: confidence = 2* sdev, 
             plt.title('Single-Task GP + Heteroscedastic Noise')
             plt.xticks(fontsize=18)
             plt.yticks(fontsize=18)
             plt.xlabel(r'$\mathbf{q}$',fontsize=17)
             plt.ylabel(r'$y$',fontsize=17)
             plt.show()
    #
    #
    #-------SETTINGS------------------------------
    n=127       #number of training data
    nGQtest=50   #number of test points (=Gauss Quadrature points)
    qBound=[0,1]   #range of input
    #type of the noise in the data
    noiseType='hetero'   #'homo'=homoscedastic, 'hetero'=heterscedastic
    #GPR options
    nIter_gpr=800      #number of iterations in optimization of hyperparameters
    lr_gpr   =0.1      #learning rate for the optimizaer of the hyperparameters    
    #number of samples drawn from GPR surrogate to construct estimates for moments of f(q)
    nMC=1000
    #---------------------------------------------    
    #(1) Generate synthetic training data
    qTrain,yTrain,noiseSdev=trainData(qBound,n,noiseType)
    #(2) Probabilistic gPCE 
    #   (a) make the dictionary
    PpceDict={'nGQtest':nGQtest,'qBound':qBound,'nIter_gpr':nIter_gpr,'lr_gpr':lr_gpr,'nMC':nMC}
    #   (b) call the method
    fMean_samples,fVar_samples,optOut=Ppce_LegUnif_1d_cnstrct(qTrain,yTrain,noiseSdev,PpceDict)
    #(3) postprocess
    #   (a) plot the GPR surrogate along with response from the exact simulator    
    gpr1D_plotter(optOut['post_f'],optOut['post_obs'],qTrain,yTrain,optOut['qTest'],fEx(optOut['qTest']))
    #   (b) plot histogram and pdf of the mean and variance distribution 
    pdfHisto.pdfFit_uniVar(fMean_samples,True,[])
    pdfHisto.pdfFit_uniVar(fVar_samples,True,[])
    #   (c) compare the exact moments with estimated values by Ppce
    fMean_ex,fVar_ex=analyticTestFuncs.fEx1D_moments(qBound)
    fMean_mean=fMean_samples.mean()
    fMean_sdev=fMean_samples.std()
    fVar_mean=fVar_samples.mean()
    fVar_sdev=fVar_samples.std()
    print(writeUQ.printRepeated('-', 80))
    print('>> Exact mean(f) = %g' %fMean_ex)
    print('   Ppce estimated: E[mean(f)] = %g , sdev[mean(f)] = %g' %(fMean_mean,fMean_sdev))
    print('>> Exact Var(f) = %g' %fVar_ex)
    print('   Ppce estimated: E[Var(f)] = %g , sdev[Var(f)] = %g' %(fVar_mean,fVar_sdev))
	
